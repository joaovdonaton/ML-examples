{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec2e6fd0-f3c2-4db8-abbc-a5d99dce6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# TODO: figure out where I want to do data normalization, decide what features to keep or not\n",
    "# idea is we can normalize everything first in the raw_da               taset, then individually convert to tensors and use get_dummies for categorical\n",
    "\n",
    "RAW_DATA_PATH = \"./data/UNSW-NB15_?.csv\" # replace ? with 1,2,3,4\n",
    "FEATURE_PATH = \"./data/NUSW-NB15_features.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e3b5680-8020-4ba3-b26d-e6f82a59e3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_159912/961960945.py:21: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(RAW_DATA_PATH.replace('?', str(i)), names=column_names)\n",
      "/tmp/ipykernel_159912/961960945.py:21: DtypeWarning: Columns (3,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(RAW_DATA_PATH.replace('?', str(i)), names=column_names)\n"
     ]
    }
   ],
   "source": [
    "# Read datasets (see README.md for source)\n",
    "\n",
    "features = pd.read_csv(FEATURE_PATH, encoding='cp1252').drop(columns=['No.'])\n",
    "column_names = list(features['Name'])\n",
    "column_names[column_names.index('ct_src_ ltm')] = 'ct_src_ltm'\n",
    "\n",
    "column_types = {}\n",
    "\n",
    "# for some reason there is some weirdness columns like ports being in hexadecimal sometimes?\n",
    "for ind, row in features.iterrows():\n",
    "    typ = row['Type '].lower()\n",
    "    if typ == 'nominal':\n",
    "        column_types[row['Name']] = str\n",
    "    elif typ == 'integer' or typ == 'timestamp' or typ == 'binary':\n",
    "        column_types[row['Name']] = np.int64\n",
    "    elif typ == 'float':\n",
    "        column_types[row['Name']] = np.float64\n",
    "\n",
    "dfs = []\n",
    "for i in range(1,5):\n",
    "    temp = pd.read_csv(RAW_DATA_PATH.replace('?', str(i)), names=column_names)\n",
    "\n",
    "    dfs.append(temp)\n",
    "\n",
    "raw_data = pd.concat(dfs)\n",
    "\n",
    "# sport and dport are string instead of int because there are some weird entries \n",
    "# exclude rows with '-' ports and convert hex ports to int \n",
    "exclude_ind = []\n",
    "\n",
    "for ind, d in raw_data.iterrows():\n",
    "    if d['sport'] == '-' or d['dsport'] == '-':\n",
    "        exclude_ind.append(ind)\n",
    "\n",
    "raw_data = raw_data.drop(exclude_ind)\n",
    "\n",
    "# convert the hex values to ints\n",
    "def convert_to_int(v): \n",
    "    if type(v) == str and v.startswith('0x'):\n",
    "        return int(v, 16)\n",
    "    return v\n",
    "\n",
    "raw_data['sport'] = raw_data['sport'].apply(convert_to_int).astype(np.int64)\n",
    "raw_data['dsport'] = raw_data['dsport'].apply(convert_to_int).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70eff186-8a36-4cbe-baba-94e4d9f68df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ct_srv_src          67\n",
       "ct_srv_dst          67\n",
       "ct_dst_ltm          67\n",
       "ct_src_ltm          67\n",
       "ct_src_dport_ltm    67\n",
       "ct_dst_sport_ltm    60\n",
       "ct_dst_src_ltm      67\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize and filter\n",
    "# For some reason there is one dsport > 65535 (like 1)\n",
    "raw_data = raw_data[raw_data['dsport'] <= 65535]\n",
    "raw_data['ct_ftp_cmd'] = raw_data['ct_ftp_cmd'].apply(lambda x: 0 if x == ' ' else int(x)).astype(np.int64)\n",
    "\n",
    "# ok now lets normalize certain columns\n",
    "# NOTE: stime and ltime columns werent normalized because we wont be needing them for the model, but we still need them to compute the sequences\n",
    "# TODO: look into maybe applying log transforms to some of these\n",
    "normalize_these = ['sport', 'dsport', 'dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'Sload', 'Dload', 'Spkts',\n",
    "                   'Dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len',\n",
    "                  'Sjit', 'Djit', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat', 'ct_state_ttl', 'ct_ftp_cmd']\n",
    "raw_data_normalized = raw_data.copy()\n",
    "raw_data_normalized[normalize_these] = scaler.fit_transform(raw_data[normalize_these])\n",
    "\n",
    "# deal with NaN situation in column ct_flw_http_mthd\n",
    "raw_data_normalized['ct_flw_http_mthd_is_nan'] = raw_data_normalized['ct_flw_http_mthd'].isna()\n",
    "raw_data_normalized['ct_flw_http_mthd'] = raw_data_normalized['ct_flw_http_mthd'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "\n",
    "# deal with is_ftp_login having NaN and 2/4 values (type is supposedly binary)\n",
    "raw_data_normalized['is_ftp_login_is_ambiguous'] = ((raw_data_normalized['is_ftp_login'].isna()) | (raw_data_normalized['is_ftp_login'] == 2))\n",
    "raw_data_normalized['is_ftp_login'] = raw_data_normalized['is_ftp_login'].apply(lambda x: 0 if pd.isna(x) or x == 2 or x == 4 else x)\n",
    "\n",
    "raw_data_normalized[['ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd3aa08b-b778-408f-8a66-3aa5f56ef039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to break out data into sequences\n",
    "TIME_WINDOW = 60 * 2 \n",
    "\n",
    "raw_data = raw_data.sort_values(by=['Stime'])\n",
    "\n",
    "groups = raw_data.groupby(by=['srcip', 'dstip']) # should be same order as in original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc95ddc-720e-4427-bca4-a09bfc5ab134",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seq = {}\n",
    "for name, g in groups:\n",
    "    sequences = []\n",
    "    window_start_time = 0\n",
    "    current_seq = []\n",
    "    for ind, data in g.iterrows():\n",
    "        if window_start_time == 0:\n",
    "            window_start_time = data['Stime']\n",
    "        \n",
    "        current_seq.append(data)\n",
    "    \n",
    "        if (data['Stime']-window_start_time) >= TIME_WINDOW:\n",
    "            window_start_time = 0\n",
    "            sequences.append(current_seq)\n",
    "            current_seq = []\n",
    "\n",
    "    all_seq[name] = sequences\n",
    "    #print(f'{name} has {len(sequences)} sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9956949-d651-4786-955e-cc7a39822f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 79616 sequences in our dataset\n",
      "Out of these, 9859 contain attacks\n"
     ]
    }
   ],
   "source": [
    "# Analysis and filtering of sequence data\n",
    "MIN_SEQ_COUNT = 30 # want to exclude IP pairs that have less than 30 sequences, since these may not provide enough information to be relevant for training\n",
    "\n",
    "raw_sequences = []\n",
    "attack_indices = []\n",
    "for k, v in all_seq.items():\n",
    "    seq_count = len(v)\n",
    "    \n",
    "    lengths = [len(d) for d in v]\n",
    "    if len(lengths) > MIN_SEQ_COUNT:\n",
    "        #print(f'{k}: {len(v)} total sequences, {min(lengths)} min length, {max(lengths)} max length, {statistics.mean(lengths):.2f} average length')\n",
    "\n",
    "        for s in v:\n",
    "            raw_sequences.append(s)\n",
    "            \n",
    "            for con in s:\n",
    "                if con['Label'] == 1:\n",
    "                    attack_indices.append(len(raw_sequences)-1)\n",
    "                    break\n",
    "\n",
    "print(f'We have {len(raw_sequences)} sequences in our dataset')\n",
    "print(f'Out of these, {len(attack_indices)} contain attacks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4852707c-c09a-450d-881b-9dd403a290e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "def raw_sequence_2_tensor(seq):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86177779-df5b-4494-90c9-f4780f1664a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, attack_inds):\n",
    "        self.raw_sequences = sequences\n",
    "        self.raw_labels = []\n",
    "\n",
    "        for index, s in enumerate(self.raw_sequences):\n",
    "            self.raw_labels.append(1 if index in attack_inds else 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.raw_sequences[index], self.raw_labels[index]\n",
    "\n",
    "\n",
    "ds = SequenceDataset(raw_sequences, attack_indices)\n",
    "raw_seq_2_tensor(ds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7378e7f1-c0a1-4d1b-92e2-b210ed79607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG CODE\n",
    "\n",
    "for j in dfs:\n",
    "    zero_cnt = 0\n",
    "    for i in j['Stime'].to_numpy():\n",
    "        if i == 0: zero_cnt += 1\n",
    "\n",
    "zero_cnt\n",
    "\n",
    "###\n",
    "for i in range(len(column_names)):\n",
    "    print(f'{column_names[i]} - {dfs[0][column_names[i]].dtype} {dfs[1][column_names[i]].dtype} {dfs[2][column_names[i]].dtype} {dfs[3][column_names[i]].dtype}')\n",
    "\n",
    "\n",
    "###\n",
    "for c in column_names:\n",
    "    print(f'Column {c} has type {raw_data[c].dtype}')\n",
    "\n",
    "### sequence generation\n",
    "g = groups.get_group(('59.166.0.0', '149.171.126.6'))\n",
    "start_times = g['Stime']\n",
    "\n",
    "sequences = []\n",
    "window_start_time = 0\n",
    "current_seq = []\n",
    "for ind, data in g.iterrows():\n",
    "    if window_start_time == 0:\n",
    "        window_start_time = data['Stime']\n",
    "    \n",
    "    #print(f'{ind} is {data['Stime']}')\n",
    "\n",
    "    current_seq.append(data)\n",
    "\n",
    "    if (data['Stime']-window_start_time) >= TIME_WINDOW:\n",
    "        window_start_time = 0\n",
    "        sequences.append(current_seq)\n",
    "        current_seq = []\n",
    "\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbaeea93-206b-4987-9b29-8311a3cedb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names.index('ct_src_ ltm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b70b0f1-5777-4a6f-a28b-5ffb4aed3744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
