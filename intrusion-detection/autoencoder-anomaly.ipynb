{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "159345af-4f30-405f-9500-d153e8f9925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/joao/.cache/kagglehub/datasets/hassan06/nslkdd/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# get dataset\n",
    "path = kagglehub.dataset_download(\"hassan06/nslkdd\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "train_name, test_name = 'KDDTrain+.arff', 'KDDTest+.arff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "644607da-30c7-4bb7-bd6a-98181e629890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# couldn't get arff loading libaries to work, so I'll do it manually\n",
    "def parse_arff(p):\n",
    "    with open(p, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        attribute_strings = [l.replace('\\n', '').replace('@attribute ', '') for l in lines if l.startswith('@attribute')]\n",
    "\n",
    "        attributes = {}\n",
    "        for s in attribute_strings:\n",
    "            # we only have attribute type real or categorical in this dataset\n",
    "            att_name = s[0:s.find(' ')].replace('\\'', '')\n",
    "            if s.endswith('real'):\n",
    "                attributes[att_name] = 'real'\n",
    "            else:\n",
    "                attributes[att_name] = eval(s[s.find(' ')+1:])\n",
    "\n",
    "        data_ind = -1 # data starts at this index\n",
    "        for i in range(len(lines)):\n",
    "            if lines[i].find('@data') != -1:\n",
    "                data_ind = i+1\n",
    "        \n",
    "        data = [] \n",
    "        positional_attribs = list(attributes.items()) # python 3.7+ guarantees dict order of insertion\n",
    "        for d in lines[data_ind:]:\n",
    "            attribs = d.replace('\\n', '').split(',')\n",
    "            row = []\n",
    "            for i in range(len(attribs)):\n",
    "                if positional_attribs[i][1] == 'real' or str(positional_attribs[i][1]) == '{\\'0\\', \\'1\\'}':\n",
    "                    row.append(float(attribs[i]))\n",
    "                else:\n",
    "                    row.append(str(attribs[i]))\n",
    "            data.append(row)\n",
    "\n",
    "    return attributes, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a40db749-9534-41cf-b860-6055e59bc4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes, train_data = parse_arff(path+'/'+train_name)\n",
    "\n",
    "train_data_df = pd.DataFrame(train_data, columns=list(attributes.keys()))\n",
    "\n",
    "# exclude anomalous entries for encoder training\n",
    "train_data_df = train_data_df[train_data_df['class'] == 'normal']\n",
    "train_data_df = train_data_df.drop(columns=['class'])\n",
    "\n",
    "# one hot encode categorical data\n",
    "train_data_df['service'] = pd.Categorical(train_data_df['service'], categories=attributes['service'])\n",
    "train_data_df['flag'] = pd.Categorical(train_data_df['flag'], categories=attributes['flag'])\n",
    "train_data_df = pd.get_dummies(train_data_df, columns=['protocol_type', 'service', 'flag'])\n",
    "\n",
    "#train_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea12ae78-7fd6-4bf6-a1a4-e80abfcde8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for column 0 (duration) std is 1304.44: 545\n",
      "\t z-score of 6 corresponds to 7826.65\n",
      "for column 1 (src_bytes) std is 418110.03: 16\n",
      "\t z-score of 6 corresponds to 2508660.18\n",
      "for column 2 (dst_bytes) std is 65462.33: 71\n",
      "\t z-score of 6 corresponds to 392773.99\n",
      "for column 3 (land) std is 0.01: 7\n",
      "\t z-score of 6 corresponds to 0.06\n",
      "for column 5 (urgent) std is 0.02: 6\n",
      "\t z-score of 6 corresponds to 0.10\n",
      "for column 6 (hot) std is 2.31: 520\n",
      "\t z-score of 6 corresponds to 13.85\n",
      "for column 7 (num_failed_logins) std is 0.05: 68\n",
      "\t z-score of 6 corresponds to 0.30\n",
      "for column 8 (logged_in) std is 0.45: 0\n",
      "\t z-score of 6 corresponds to 2.72\n",
      "for column 9 (num_compromised) std is 32.74: 43\n",
      "\t z-score of 6 corresponds to 196.46\n",
      "for column 10 (root_shell) std is 0.05: 137\n",
      "\t z-score of 6 corresponds to 0.27\n",
      "for column 11 (su_attempted) std is 0.06: 79\n",
      "\t z-score of 6 corresponds to 0.37\n",
      "for column 12 (num_root) std is 33.37: 45\n",
      "\t z-score of 6 corresponds to 200.21\n",
      "for column 13 (num_file_creations) std is 0.65: 81\n",
      "\t z-score of 6 corresponds to 3.91\n",
      "for column 14 (num_shells) std is 0.03: 39\n",
      "\t z-score of 6 corresponds to 0.16\n",
      "for column 15 (num_access_files) std is 0.14: 361\n",
      "\t z-score of 6 corresponds to 0.81\n",
      "for column 17 (is_host_login) std is 0.00: 1\n",
      "\t z-score of 6 corresponds to 0.02\n",
      "for column 18 (is_guest_login) std is 0.11: 873\n",
      "\t z-score of 6 corresponds to 0.68\n",
      "for column 19 (count) std is 54.03: 182\n",
      "\t z-score of 6 corresponds to 324.15\n",
      "for column 20 (srv_count) std is 60.18: 134\n",
      "\t z-score of 6 corresponds to 361.09\n",
      "for column 28 (dst_host_count) std is 101.78: 0\n",
      "\t z-score of 6 corresponds to 610.71\n",
      "for column 29 (dst_host_srv_count) std is 92.61: 0\n",
      "\t z-score of 6 corresponds to 555.65\n"
     ]
    }
   ],
   "source": [
    "# before normalizing, let's see if we don't have any huge outliers since we're doing minmax scaling\n",
    "# we can exclude our hot encoded ones, and also ignore any of the rate features or booleans\n",
    "columns = list(train_data_df.columns)\n",
    "excluded_indexes = [i for i in range(columns.index('protocol_type_icmp'), len(columns))]\n",
    "excluded_indexes.append([])\n",
    "\n",
    "train_data_np = train_data_df.to_numpy()\n",
    "for i in range(len(columns)):\n",
    "    if i not in excluded_indexes:\n",
    "        col = train_data_np[:, i]\n",
    "        std = col.std()\n",
    "        if std != 0 and columns[i].find('rate') == -1: \n",
    "            z_col = col/std\n",
    "            z_col = z_col[(z_col > 6) | (z_col < -6)] # count entries that are beyond 6 z-score range\n",
    "            print(f'for column {i} ({columns[i]}) std is {std:.2f}:', len(z_col))\n",
    "            print(f'\\t z-score of 6 corresponds to {std*6:.2f}')\n",
    "    \n",
    "            # automatically filter from out dataframe\n",
    "            train_data_df = train_data_df[train_data_df[columns[i]] <= std*6]\n",
    "        \n",
    "# investigate by plotting our samples\n",
    "#plt.scatter(np.linspace(0, 100000, len(train_data_df['duration'])), train_data_df['duration'], s=1)\n",
    "\n",
    "# maybe log transform some of the fields that have 0 or large values\n",
    "train_data_df['duration'] = np.log(train_data_df['duration']+0.001)\n",
    "train_data_df['src_bytes'] = np.log(train_data_df['src_bytes']+0.001)\n",
    "train_data_df['dst_bytes'] = np.log(train_data_df['dst_bytes']+0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef652808-3d58-4b05-ac00-519272cd7bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65162\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c46afc5-b23f-476d-95f4-e702d5ca571d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65162, 122)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize\n",
    "data = scaler.fit_transform(train_data_df)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "641b598d-772a-459a-82f6-418476542228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use pytorch to implement our model\n",
    "# source for architecture: https://pmc.ncbi.nlm.nih.gov/articles/PMC8272075/pdf/sensors-21-04294.pdf\n",
    "# Apparently, it looks like different depth and hidden layer neuron numbers achieve very similar results on this dataset\n",
    "# We'll go for a symmetric autoencoder with depth of 5 with 64 neurons on the first hidden layer (subsequent layers divide number of neurons by 2)\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, latent_dim),\n",
    "            # torch.nn.ReLU() MAYBE?\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, input_dim),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db2ad599-73aa-4bc0-afb4-a6898efec57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5121, 0.4626, 0.4720, 0.5340, 0.4542, 0.4727, 0.4755, 0.4666, 0.5350,\n",
       "        0.5373, 0.4960, 0.5084, 0.5191, 0.4951, 0.5448, 0.5674, 0.5186, 0.4893,\n",
       "        0.4677, 0.4663, 0.5431, 0.5050, 0.4585, 0.5016, 0.4916, 0.4560, 0.5265,\n",
       "        0.4868, 0.5285, 0.5040, 0.5436, 0.5016, 0.4953, 0.5133, 0.4617, 0.5343,\n",
       "        0.5017, 0.4994, 0.4599, 0.5280, 0.4831, 0.4723, 0.4829, 0.4633, 0.4612,\n",
       "        0.4808, 0.4948, 0.4489, 0.4599, 0.4908, 0.5101, 0.4962, 0.4734, 0.5055,\n",
       "        0.5069, 0.5067, 0.5068, 0.5057, 0.4919, 0.4904, 0.5016, 0.5162, 0.5244,\n",
       "        0.4988, 0.4849, 0.4495, 0.4814, 0.4917, 0.5354, 0.5167, 0.4880, 0.4826,\n",
       "        0.5295, 0.4465, 0.4943, 0.5219, 0.5291, 0.5014, 0.4618, 0.5002, 0.5109,\n",
       "        0.4402, 0.4630, 0.4857, 0.5010, 0.5176, 0.5168, 0.4984, 0.4886, 0.4819,\n",
       "        0.5289, 0.5505, 0.4936, 0.4985, 0.4611, 0.4874, 0.5482, 0.4771, 0.4716,\n",
       "        0.4840, 0.5507, 0.5419, 0.5079, 0.5142, 0.5125, 0.5064, 0.5307, 0.5388,\n",
       "        0.4718, 0.5283, 0.4889, 0.4906, 0.4833, 0.5565, 0.5271, 0.4889, 0.5142,\n",
       "        0.5201, 0.5612, 0.5207, 0.5524, 0.5307], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Autoencoder(data.shape[1], 3)\n",
    "model.forward(torch.from_numpy(data[0]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82535d41-0d19-4a09-a94e-2723b230475b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
