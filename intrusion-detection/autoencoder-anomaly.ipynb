{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "159345af-4f30-405f-9500-d153e8f9925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/joao/.cache/kagglehub/datasets/hassan06/nslkdd/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# get dataset\n",
    "path = kagglehub.dataset_download(\"hassan06/nslkdd\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "train_name, test_name = 'KDDTrain+.arff', 'KDDTest+.arff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "644607da-30c7-4bb7-bd6a-98181e629890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# couldn't get arff loading libaries to work, so I'll do it manually\n",
    "def parse_arff(p):\n",
    "    with open(p, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        attribute_strings = [l.replace('\\n', '').replace('@attribute ', '') for l in lines if l.startswith('@attribute')]\n",
    "\n",
    "        attributes = {}\n",
    "        for s in attribute_strings:\n",
    "            # we only have attribute type real or categorical in this dataset\n",
    "            att_name = s[0:s.find(' ')].replace('\\'', '')\n",
    "            if s.endswith('real'):\n",
    "                attributes[att_name] = 'real'\n",
    "            else:\n",
    "                attributes[att_name] = eval(s[s.find(' ')+1:])\n",
    "\n",
    "        data_ind = -1 # data starts at this index\n",
    "        for i in range(len(lines)):\n",
    "            if lines[i].find('@data') != -1:\n",
    "                data_ind = i+1\n",
    "        \n",
    "        data = [] \n",
    "        positional_attribs = list(attributes.items()) # python 3.7+ guarantees dict order of insertion\n",
    "        for d in lines[data_ind:]:\n",
    "            attribs = d.replace('\\n', '').split(',')\n",
    "            row = []\n",
    "            for i in range(len(attribs)):\n",
    "                if positional_attribs[i][1] == 'real' or str(positional_attribs[i][1]) == '{\\'0\\', \\'1\\'}':\n",
    "                    row.append(float(attribs[i]))\n",
    "                else:\n",
    "                    row.append(str(attribs[i]))\n",
    "            data.append(row)\n",
    "\n",
    "    return attributes, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a40db749-9534-41cf-b860-6055e59bc4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes, train_data = parse_arff(path+'/'+train_name)\n",
    "\n",
    "train_data_df = pd.DataFrame(train_data, columns=list(attributes.keys()))\n",
    "\n",
    "# exclude anomalous entries for encoder training\n",
    "train_data_df = train_data_df[train_data_df['class'] == 'normal']\n",
    "train_data_df = train_data_df.drop(columns=['class'])\n",
    "\n",
    "# one hot encode categorical data\n",
    "train_data_df['service'] = pd.Categorical(train_data_df['service'], categories=attributes['service'])\n",
    "train_data_df['flag'] = pd.Categorical(train_data_df['flag'], categories=attributes['flag'])\n",
    "train_data_df = pd.get_dummies(train_data_df, columns=['protocol_type', 'service', 'flag'])\n",
    "\n",
    "#train_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea12ae78-7fd6-4bf6-a1a4-e80abfcde8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for column 0 (duration) std is 1304.44: 545\n",
      "\t z-score of 6 corresponds to 7826.65\n",
      "for column 1 (src_bytes) std is 418110.03: 16\n",
      "\t z-score of 6 corresponds to 2508660.18\n",
      "for column 2 (dst_bytes) std is 65462.33: 71\n",
      "\t z-score of 6 corresponds to 392773.99\n",
      "for column 3 (land) std is 0.01: 7\n",
      "\t z-score of 6 corresponds to 0.06\n",
      "for column 5 (urgent) std is 0.02: 6\n",
      "\t z-score of 6 corresponds to 0.10\n",
      "for column 6 (hot) std is 2.31: 520\n",
      "\t z-score of 6 corresponds to 13.85\n",
      "for column 7 (num_failed_logins) std is 0.05: 68\n",
      "\t z-score of 6 corresponds to 0.30\n",
      "for column 8 (logged_in) std is 0.45: 0\n",
      "\t z-score of 6 corresponds to 2.72\n",
      "for column 9 (num_compromised) std is 32.74: 43\n",
      "\t z-score of 6 corresponds to 196.46\n",
      "for column 10 (root_shell) std is 0.05: 137\n",
      "\t z-score of 6 corresponds to 0.27\n",
      "for column 11 (su_attempted) std is 0.06: 79\n",
      "\t z-score of 6 corresponds to 0.37\n",
      "for column 12 (num_root) std is 33.37: 45\n",
      "\t z-score of 6 corresponds to 200.21\n",
      "for column 13 (num_file_creations) std is 0.65: 81\n",
      "\t z-score of 6 corresponds to 3.91\n",
      "for column 14 (num_shells) std is 0.03: 39\n",
      "\t z-score of 6 corresponds to 0.16\n",
      "for column 15 (num_access_files) std is 0.14: 361\n",
      "\t z-score of 6 corresponds to 0.81\n",
      "for column 17 (is_host_login) std is 0.00: 1\n",
      "\t z-score of 6 corresponds to 0.02\n",
      "for column 18 (is_guest_login) std is 0.11: 873\n",
      "\t z-score of 6 corresponds to 0.68\n",
      "for column 19 (count) std is 54.03: 182\n",
      "\t z-score of 6 corresponds to 324.15\n",
      "for column 20 (srv_count) std is 60.18: 134\n",
      "\t z-score of 6 corresponds to 361.09\n",
      "for column 28 (dst_host_count) std is 101.78: 0\n",
      "\t z-score of 6 corresponds to 610.71\n",
      "for column 29 (dst_host_srv_count) std is 92.61: 0\n",
      "\t z-score of 6 corresponds to 555.65\n"
     ]
    }
   ],
   "source": [
    "# before normalizing, let's see if we don't have any huge outliers since we're doing minmax scaling\n",
    "# we can exclude our hot encoded ones, and also ignore any of the rate features or booleans\n",
    "columns = list(train_data_df.columns)\n",
    "excluded_indexes = [i for i in range(columns.index('protocol_type_icmp'), len(columns))]\n",
    "excluded_indexes.append([])\n",
    "\n",
    "train_data_np = train_data_df.to_numpy()\n",
    "for i in range(len(columns)):\n",
    "    if i not in excluded_indexes:\n",
    "        col = train_data_np[:, i]\n",
    "        std = col.std()\n",
    "        if std != 0 and columns[i].find('rate') == -1: \n",
    "            z_col = col/std\n",
    "            z_col = z_col[(z_col > 6) | (z_col < -6)] # count entries that are beyond 6 z-score range\n",
    "            print(f'for column {i} ({columns[i]}) std is {std:.2f}:', len(z_col))\n",
    "            print(f'\\t z-score of 6 corresponds to {std*6:.2f}')\n",
    "    \n",
    "            # automatically filter from out dataframe\n",
    "            train_data_df = train_data_df[train_data_df[columns[i]] <= std*6]\n",
    "        \n",
    "# investigate by plotting our samples\n",
    "#plt.scatter(np.linspace(0, 100000, len(train_data_df['duration'])), train_data_df['duration'], s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef652808-3d58-4b05-ac00-519272cd7bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65162\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46afc5-b23f-476d-95f4-e702d5ca571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "scaler.fit_transform(train_data_df)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b598d-772a-459a-82f6-418476542228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use pytorch to implement our model\n",
    "# source for architecture: https://pmc.ncbi.nlm.nih.gov/articles/PMC8272075/pdf/sensors-21-04294.pdf\n",
    "# Apparently, it looks like different depth and hidden layer neuron numbers achieve very similar results on this dataset\n",
    "# We'll go for a symmetric autoencoder with depth of 5 with 64 neurons on the first hidden layer (subsequent layers divide number of neurons by 2)\n",
    "# Paper states latent space size of 3 works best with this setup\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
